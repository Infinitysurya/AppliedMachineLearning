{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "vsesWgFJpCZu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bea298f1-ae82-4cc1-ed04-b0b6d9052b39"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_chicken_dir = \"/content/drive/MyDrive/data/chicken-images/data/train\"\n",
        "train_duck_dir = \"/content/drive/MyDrive/data/duck-images/data/train\"\n",
        "val_chicken_dir = \"/content/drive/MyDrive/data/chicken-images/data/val\"\n",
        "val_duck_dir = \"/content/drive/MyDrive/data/duck-images/data/val\"\n",
        "test_chicken_dir = \"/content/drive/MyDrive/data/chicken-images/data/test\"\n",
        "test_duck_dir = \"/content/drive/MyDrive/data/duck-images/data/test\""
      ],
      "metadata": {
        "id": "pc9-6ZgXvpse"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "class DuckChickenDataset(Dataset):\n",
        "    def __init__(self, chicken_dir, duck_dir, transform=None):\n",
        "        self.transform = transform\n",
        "        self.samples = []\n",
        "\n",
        "        for img_name in os.listdir(chicken_dir):\n",
        "            self.samples.append((os.path.join(chicken_dir, img_name), 0))  # label 0: chicken\n",
        "\n",
        "        for img_name in os.listdir(duck_dir):\n",
        "            self.samples.append((os.path.join(duck_dir, img_name), 1))  # label 1: duck\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, label = self.samples[idx]\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n"
      ],
      "metadata": {
        "id": "CYYhCcwDvq23"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "train_dataset = DuckChickenDataset(train_chicken_dir, train_duck_dir, transform)\n",
        "val_dataset = DuckChickenDataset(val_chicken_dir, val_duck_dir, transform)\n",
        "test_dataset = DuckChickenDataset(test_chicken_dir, test_duck_dir, transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32)\n"
      ],
      "metadata": {
        "id": "tlqonGc9vxVd"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = models.resnet18(pretrained=True)\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "model.fc = nn.Linear(model.fc.in_features, 2)  # 2 classes\n",
        "model = model.to(device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PaxP62esv1NI",
        "outputId": "bde2dae4-5e25-498d-9891-3be08233433c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 93.2MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.fc.parameters(), lr=0.001)\n",
        "\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "    return correct / total\n",
        "\n",
        "def train(model, train_loader, val_loader, epochs=5):\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss, correct, total = 0.0, 0, 0\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "        val_acc = evaluate(model, val_loader)\n",
        "        print(f\"Epoch {epoch+1}: Loss={running_loss:.4f}, Train Acc={correct/total:.4f}, Val Acc={val_acc:.4f}\")\n",
        "\n",
        "train(model, train_loader, val_loader, epochs=5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZjNWaxE5v7aP",
        "outputId": "5e4346ce-26c8-417e-ee12-64e9177edbee"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Loss=12.0772, Train Acc=0.8105, Val Acc=0.8820\n",
            "Epoch 2: Loss=6.7523, Train Acc=0.9253, Val Acc=0.9130\n",
            "Epoch 3: Loss=4.7423, Train Acc=0.9509, Val Acc=0.9317\n",
            "Epoch 4: Loss=4.4988, Train Acc=0.9509, Val Acc=0.9317\n",
            "Epoch 5: Loss=3.7562, Train Acc=0.9654, Val Acc=0.8882\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "model.eval()\n",
        "all_preds, all_labels = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        inputs = inputs.to(device)\n",
        "        outputs = model(inputs)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.numpy())\n",
        "\n",
        "print(\"Classification Report:\\n\")\n",
        "print(classification_report(all_labels, all_preds, target_names=[\"chicken\", \"duck\"]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n4vRAyIc30Zd",
        "outputId": "81f9abb3-032e-4bc5-f5e4-cb2ea849b0ea"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     chicken       1.00      0.82      0.90       172\n",
            "        duck       0.91      1.00      0.95       310\n",
            "\n",
            "    accuracy                           0.94       482\n",
            "   macro avg       0.95      0.91      0.93       482\n",
            "weighted avg       0.94      0.94      0.93       482\n",
            "\n"
          ]
        }
      ]
    }
  ]
}